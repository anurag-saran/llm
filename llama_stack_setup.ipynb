{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first go to your ubuntu/mac os environment and install:\n",
    "1) ollama - https://ollama.com/download/linux\n",
    "2) miniconda - https://www.anaconda.com/docs/getting-started/miniconda/install\n",
    "\n",
    "After installing these we go the the llama-stack documentation quick start page - https://llama-stack.readthedocs.io/en/latest/getting_started/index.html\n",
    "We will be using the ollama distribution for local inference, Here is the guide for ollama llama stack - https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/ollama.html\n",
    "\n",
    "some important default config to note:\n",
    "\n",
    "The following environment variables can be configured but i recommend sticking to default values:\n",
    "* LLAMA_STACK_PORT: Port for the Llama Stack distribution server (default: 5001)\n",
    "* OLLAMA_URL: URL of the Ollama server (default: http://127.0.0.1:11434)\n",
    "* INFERENCE_MODEL: Inference model loaded into the Ollama server (default: meta-llama/Llama-3.2-3B-Instruct)\n",
    "* SAFETY_MODEL: Safety model loaded into the Ollama server (default: meta-llama/Llama-Guard-3-1B)\n",
    "\n",
    "first we create our conda environment for llama-stack and install llama-stack-client\n",
    "\n",
    "```bash\n",
    "yes | conda create -n stack-client python=3.10\n",
    "conda activate stack-client\n",
    "pip install llama-stack-client\n",
    "```\n",
    "\n",
    "You can then test the llama-stack-client in terminal\n",
    "\n",
    "```bash\n",
    "llama-stack-client configure --endpoint http://localhost:5001\n",
    "```\n",
    "\n",
    "You will get such a message but we don't need any api key so just enter.\n",
    "\n",
    "```\n",
    "> Enter the API key (leave empty if no key is needed):\n",
    "\n",
    "Done! You can now use the Llama Stack Client CLI with endpoint http://localhost:5001\n",
    "```\n",
    "\n",
    "```bash\n",
    "llama-stack-client models list\n",
    "```\n",
    "\n",
    "```\n",
    "Available Models\n",
    "\n",
    "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
    "┃ model_type   ┃ identifier                           ┃ provider_resource_id         ┃ metadata  ┃ provider_id ┃\n",
    "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
    "│ llm          │ meta-llama/Llama-3.2-3B-Instruct     │ llama3.2:3b-instruct-fp16    │           │ ollama      │\n",
    "└──────────────┴──────────────────────────────────────┴──────────────────────────────┴───────────┴─────────────┘\n",
    "\n",
    "Total models: 1\n",
    "```\n",
    "\n",
    "Now go create you project folder and cd into it while using in your conda stack-client environment and run these.\n",
    "\n",
    "```bash\n",
    "pip install uv\n",
    "uv pip install llama-stack\n",
    "export INFERENCE_MODEL=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "export OLLAMA_INFERENCE_MODEL=\"llama3.2:3b-instruct-fp16\"\n",
    "export SAFETY_MODEL=\"meta-llama/Llama-Guard-3-1B\"\n",
    "export OLLAMA_SAFETY_MODEL=\"llama-guard3:1b\"\n",
    "export LLAMA_STACK_PORT=5001\n",
    "llama stack build --template ollama --image-type conda\n",
    "llama stack run ./run-with-safety.yaml \\\n",
    "  --port $LLAMA_STACK_PORT \\\n",
    "  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n",
    "  --env SAFETY_MODEL=$SAFETY_MODEL \\\n",
    "  --env OLLAMA_URL=http://localhost:11434\n",
    "```\n",
    "\n",
    "NOTE: If you get an error about run-with-safety.yaml not found. You have to go download it from their github.\n",
    "link - https://github.com/meta-llama/llama-stack/blob/main/llama_stack/templates/ollama/run-with-safety.yaml\n",
    "download and put it in your llama stack project folder you made.\n",
    "\n",
    "Now from ollama install and run the default llama model that we will use for inference. (this requires 6.4GB space)\n",
    "\n",
    "first start ollama server to use it\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "to check if ollama is running open the below given url:\n",
    "\n",
    "http://127.0.0.1:11434\n",
    "\n",
    "Now in another terminal run the following to download the model and run it. don't close the server while loading model this will take some time depedning upon internet connection:\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:3b-instruct-fp16\n",
    "ollama pull llama-guard3:1b\n",
    "ollama list\n",
    "```\n",
    "\n",
    "To test the models you can run them and start asking question in terminal\n",
    "\n",
    "```bash\n",
    "ollama run llama3.2:3b-instruct-fp16 \n",
    "```\n",
    "\n",
    "To test the models you can run them and start asking question in terminal\n",
    "\n",
    "```bash\n",
    "ollama run llama-guard3:1b \n",
    "```\n",
    "\n",
    "Issues - for some reason on both windows and ubuntu only one model either the inference model or the shield will show up. I recommend keeping the scripts for bot separate.\n",
    "Also you might need to run and rerun if llama stack can't find the model. This issue was also present in both ubuntu and windows and is directly linked to ollama.\n",
    "```bash\n",
    "ollama run llama3.2:3b-instruct-fp16 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now coming to the code. Import these two libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_stack_client\n",
    "import llama_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all environment variables. If you want to use other models change the names to that model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "INFERENCE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "os.environ[\"INFERENCE_MODEL\"] = INFERENCE_MODEL\n",
    "os.environ[\"SAFETY_MODEL\"]=\"meta-llama/Llama-Guard-3-1B\"\n",
    "os.environ[\"OLLAMA_INFERENCE_MODEL\"]=\"llama3.2:3b-instruct-fp16\"\n",
    "os.environ[\"OLLAMA_SAFETY_MODEL\"]=\"llama-guard3:1b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example1: Inference using Llama stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: `bwrap` is not available. Code interpreter tool will not work correctly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">ollama</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using config \u001b[34mollama\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: <span style=\"font-weight: bold\">[]</span>\n",
       "container_image: null\n",
       "datasets: <span style=\"font-weight: bold\">[]</span>\n",
       "image_name: ollama\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">registry.db</span>\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-3B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: ollama\n",
       "  provider_model_id: all-minilm:latest\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">agents_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">huggingface_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">localfs_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">meta_reference_eval.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:11434</span>\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: <span style=\"font-weight: bold\">[]</span>\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: basic\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>raintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: llama-stack\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">trace_store.db</span>\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>rave-search\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::c</span>ode-interpreter\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">faiss_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
       "scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
       "server:\n",
       "  port: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8321</span>\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: <span style=\"font-weight: bold\">[]</span>\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: <span style=\"font-weight: bold\">[]</span>\n",
       "version: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "container_image: null\n",
       "datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "image_name: ollama\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mregistry.db\u001b[0m\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-3B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: \u001b[1;36m384\u001b[0m\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: ollama\n",
       "  provider_model_id: all-minilm:latest\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95magents_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mhuggingface_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mlocalfs_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mmeta_reference_eval.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: \u001b[4;94mhttp://localhost:11434\u001b[0m\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: basic\n",
       "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin\u001b[1;92me::b\u001b[0mraintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: llama-stack\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mtrace_store.db\u001b[0m\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot\u001b[1;92me::b\u001b[0mrave-search\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin\u001b[1;92me::c\u001b[0mode-interpreter\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mfaiss_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
       "scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "server:\n",
       "  port: \u001b[1;36m8321\u001b[0m\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "version: \u001b[32m'2'\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Available models: ---\n",
      "- meta-llama/Llama-3.2-3B-Instruct\n",
      "- all-MiniLM-L6-v2\n",
      "\n",
      "Here is a haiku about coding:\n",
      "\n",
      "Lines of code unfold\n",
      "Logic flows through digital night\n",
      "Beauty in the bits\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def create_library_client(template=\"ollama\"):\n",
    "    from llama_stack import LlamaStackAsLibraryClient\n",
    "\n",
    "    client = LlamaStackAsLibraryClient(template)\n",
    "    if not client.initialize():\n",
    "        print(\"llama stack not built properly\")\n",
    "        sys.exit(1)\n",
    "    return client\n",
    "\n",
    "\n",
    "client = (\n",
    "    create_library_client()\n",
    ")  # or create_http_client() depending on the environment you picked\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"- {m.identifier}\")\n",
    "print()\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id = INFERENCE_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about coding\"},\n",
    "    ],\n",
    ")\n",
    "print(response.completion_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: RAG from documents using Llama stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: `bwrap` is not available. Code interpreter tool will not work correctly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">ollama</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using config \u001b[34mollama\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: <span style=\"font-weight: bold\">[]</span>\n",
       "container_image: null\n",
       "datasets: <span style=\"font-weight: bold\">[]</span>\n",
       "image_name: ollama\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">registry.db</span>\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-3B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: ollama\n",
       "  provider_model_id: all-minilm:latest\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">agents_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">huggingface_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">localfs_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">meta_reference_eval.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:11434</span>\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: <span style=\"font-weight: bold\">[]</span>\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: basic\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>raintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: llama-stack\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">trace_store.db</span>\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>rave-search\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::c</span>ode-interpreter\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/asaran/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">faiss_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
       "scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
       "server:\n",
       "  port: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8321</span>\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: <span style=\"font-weight: bold\">[]</span>\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: <span style=\"font-weight: bold\">[]</span>\n",
       "version: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "container_image: null\n",
       "datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "image_name: ollama\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mregistry.db\u001b[0m\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-3B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: \u001b[1;36m384\u001b[0m\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: ollama\n",
       "  provider_model_id: all-minilm:latest\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95magents_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mhuggingface_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mlocalfs_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mmeta_reference_eval.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: \u001b[4;94mhttp://localhost:11434\u001b[0m\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: basic\n",
       "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin\u001b[1;92me::b\u001b[0mraintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: llama-stack\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mtrace_store.db\u001b[0m\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot\u001b[1;92me::b\u001b[0mrave-search\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin\u001b[1;92me::c\u001b[0mode-interpreter\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/Users/asaran/.llama/distributions/ollama/\u001b[0m\u001b[95mfaiss_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
       "scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "server:\n",
       "  port: \u001b[1;36m8321\u001b[0m\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "version: \u001b[32m'2'\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser> How to optimize memory usage in torchtune? use the knowledge_search tool to get information.\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mnowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m(query\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mtorch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m memory\u001b[0m\u001b[33m optimization\u001b[0m\u001b[33m\")\u001b[0m\u001b[33m]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'torchtune memory optimization'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text='Result 1:\\nDocument_id:num-2\\nContent: .. _memory_optimization_overview_label:\\n\\n============================\\nMemory Optimization Overview\\n============================\\n\\n**Author**: `Salman Mohammadi <https://github.com/SalmanMohammadi>`_\\n\\ntorchtune comes with a host of plug-and-play memory optimization components which give you lots of flexibility\\nto ``tune`` our recipes to your hardware. This page provides a brief glossary of these components and how you might use them.\\nTo make things easy, we\\'ve summarized these components in the following table:\\n\\n.. csv-table:: Memory optimization components\\n   :header: \"Component\", \"When to use?\"\\n   :widths: auto\\n\\n   \":ref:`glossary_precision`\", \"You\\'ll usually want to leave this as its default ``bfloat16``. It uses 2 bytes per model parameter instead of 4 bytes when using ``float32``.\"\\n   \":ref:`glossary_act_ckpt`\", \"Use when you\\'re memory constrained and want to use a larger model, batch size or context length. Be aware that it will slow down training speed.\"\\n   \":ref:`glossary_act_off`\", \"Similar to activation checkpointing, this can be used when memory constrained, but may decrease training speed. This **should** be used alongside activation checkpointing.\"\\n   \":ref:`glossary_grad_accm`\", \"Helpful when memory-constrained to simulate larger batch sizes. Not compatible with optimizer in backward. Use it when you can already fit at least one sample without OOMing, but not enough of them.\"\\n   \":ref:`glossary_low_precision_opt`\", \"Use when you want to reduce the size of the optimizer state. This is relevant when training large models and using optimizers with momentum, like Adam. Note that lower precision optimizers may reduce training stability/accuracy.\"\\n   \":ref:`glossary_opt_in_bwd`\", \"Use it when you have large gradients and can fit a large enough batch size, since this is not compatible with ``gradient_accumulation_steps``.\"\\n   \":ref:`glossary_cpu_offload`\", \"Offloads optimizer states and (optionally) gradients to CPU, and performs optimizer steps on CPU. This can be used to significantly reduce GPU memory usage at the cost of CPU RAM and training speed. Prioritize using it only if the other techniques are not enough.\"\\n   \":ref:`glossary_lora`\", \"When you want to significantly reduce the number of trainable parameters, saving gradient and optimizer memory\\n', type='text'), TextContentItem(text='Result 2:\\nDocument_id:num-2\\nContent: precision\", uses 4 bytes per model and optimizer parameter.\\n* ``bfloat16``, referred to as \"half-precision\", uses 2 bytes per model and optimizer parameter - effectively half\\n  the memory of ``fp32``, and also improves training speed. Generally, if your hardware supports training with ``bfloat16``,\\n  we recommend using it - this is the default setting for our recipes.\\n\\n.. note::\\n\\n  Another common paradigm is \"mixed-precision\" training: where model weights are in ``bfloat16`` (or ``fp16``), and optimizer\\n  states are in ``fp32``. Currently, we don\\'t support mixed-precision training in torchtune.\\n\\n*Sounds great! How do I use it?*\\n\\nSimply use the ``dtype`` flag or config entry in all our recipes! For example, to use half-precision training in ``bf16``,\\nset ``dtype=bf16``.\\n\\n.. _glossary_act_ckpt:\\n\\nActivation Checkpointing\\n------------------------\\n\\n*What\\'s going on here?*\\n\\nThe relevant section in the `PyTorch documentation <https://pytorch.org/docs/stable/checkpoint.html>`_ explains this concept well.\\nTo quote:\\n\\n  Activation checkpointing is a technique that trades compute for memory.\\n  Instead of keeping tensors needed for backward alive until they are used in\\n  gradient computation during backward, forward computation in checkpointed\\n  regions omits saving tensors for backward and recomputes them during the backward pass.\\n\\nThis setting is helpful for when you\\'re memory-constrained, especially due to larger batch sizes or longer context lengths.\\nHowever, these savings in memory come at the cost of training speed (i.e. tokens-per-second),\\nand in most cases training can slow down quite a bit as a result of this activation recomputation.\\n\\n*Sounds great! How do I use it?*\\n\\nTo enable activation checkpointing, use ``enable_activation_checkpointing=True``.\\n\\n.. _glossary_act_off:\\n\\nActivation Offloading\\n---------------------\\n\\n*What\\'s going on here?*\\n\\nYou may have just read about activation checkpointing! Similar to checkpointing, offloading is a memory\\nefficiency technique that allows saving GPU VRAM by temporarily moving activations to CPU and bringing\\nthem back when needed in the backward pass.\\n\\nSee `PyTorch autograd hook tutorial <https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html#saving-tensors-to-cpu>`_\\nfor more details about how this is implemented through :\\n', type='text'), TextContentItem(text='Result 3:\\nDocument_id:num-1\\nContent:  be used with recipe scripts too, in case you want to make more custom changes\\n    that cannot be achieved by directly modifying existing configurable parameters. For more on :ref:`tune cp <tune_cp_cli_label>` see the section on\\n    :ref:`modifying configs <tune_cp_label>` in our \":ref:`finetune_llama_label`\" tutorial.\\n\\nOnce training is complete, the model checkpoints will be saved and their locations will be logged. For\\nLoRA fine-tuning, the final checkpoint will contain the merged weights, and a copy of just the (much smaller) LoRA weights\\nwill be saved separately.\\n\\nIn our experiments, we observed a peak memory usage of 18.5 GB. The default config can be trained on a consumer GPU with 24 GB VRAM.\\n\\nIf you have multiple GPUs available, you can run the distributed version of the recipe.\\ntorchtune makes use of the `FSDP <https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`_ APIs from PyTorch Distributed\\nto shard the model, optimizer states, and gradients. This should enable you to increase your batch size, resulting in faster overall training.\\nFor example, on two devices:\\n\\n.. code-block:: bash\\n\\n    tune run --nproc_per_node 2 lora_finetune_distributed --config llama3/8B_lora\\n\\nFinally, if we want to use even less memory, we can leverage torchtune\\'s QLoRA recipe via:\\n\\n.. TODO (SalmanMohammadi) ref qlora recipe page\\n\\n.. code-block:: bash\\n\\n    tune run lora_finetune_single_device --config llama3/8B_qlora_single_device\\n\\nSince our default configs enable full bfloat16 training, all of the above commands can be run with\\ndevices having at least 24 GB of VRAM, and in fact the QLoRA recipe should have peak allocated memory\\nbelow 10 GB. You can also experiment with different configurations of LoRA and QLoRA, or even run a full fine-tune.\\nTry it out!\\n\\n|\\n\\nEvaluating fine-tuned Llama3-8B models with EleutherAI\\'s Eval Harness\\n---------------------------------------------------------------------\\n\\nNow that we\\'ve fine-tuned our model, what\\'s next? Let\\'s take our LoRA-finetuned model from the\\npreceding section and look at a couple different ways we can evaluate its performance on the tasks we care about.\\n\\nFirst, torchtune\\n', type='text'), TextContentItem(text='Result 4:\\nDocument_id:num-3\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most commodity GPUs which support `bfloat16 <https://\\n', type='text'), TextContentItem(text=\"Result 5:\\nDocument_id:num-2\\nContent:  ``offload_gradients=True``, as gradients are cleared on GPU every backward pass.\\n* This optimizer works by keeping a copy of parameters and pre-allocating gradient memory on CPU. Therefore, expect your RAM usage to increase by 4x model size.\\n* This optimizer is only supported for single-device recipes. To use CPU-offloading in distributed recipes, use ``fsdp_cpu_offload=True`` instead. See :class:`torch.distributed.fsdp.FullyShardedDataParallel` for more details and `FSDP1 vs FSDP2 <https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md>`_ to see how they differ.\\n\\n\\n.. _glossary_peft:\\n\\nParameter Efficient Fine-Tuning (PEFT)\\n--------------------------------------\\n\\n.. _glossary_lora:\\n\\nLow Rank Adaptation (LoRA)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n*What's going on here?*\\n\\nYou can read our tutorial on :ref:`finetuning Llama2 with LoRA<lora_finetune_label>` to understand how LoRA works, and how to use it.\\nSimply stated, LoRA greatly reduces the number of trainable parameters, thus saving significant gradient and optimizer\\nmemory during training.\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: List[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mHere\u001b[0m\u001b[33m is\u001b[0m\u001b[33m the\u001b[0m\u001b[33m rewritten\u001b[0m\u001b[33m text\u001b[0m\u001b[33m in\u001b[0m\u001b[33m a\u001b[0m\u001b[33m more\u001b[0m\u001b[33m readable\u001b[0m\u001b[33m format\u001b[0m\u001b[33m with\u001b[0m\u001b[33m proper\u001b[0m\u001b[33m headings\u001b[0m\u001b[33m and\u001b[0m\u001b[33m formatting\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mIntroduction\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mThis\u001b[0m\u001b[33m document\u001b[0m\u001b[33m provides\u001b[0m\u001b[33m an\u001b[0m\u001b[33m overview\u001b[0m\u001b[33m of\u001b[0m\u001b[33m various\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m for\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33muning\u001b[0m\u001b[33m large\u001b[0m\u001b[33m language\u001b[0m\u001b[33m models\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m Low\u001b[0m\u001b[33m Rank\u001b[0m\u001b[33m Adapt\u001b[0m\u001b[33mation\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and\u001b[0m\u001b[33m Parameter\u001b[0m\u001b[33m Efficient\u001b[0m\u001b[33m Fine\u001b[0m\u001b[33m-T\u001b[0m\u001b[33muning\u001b[0m\u001b[33m (\u001b[0m\u001b[33mPE\u001b[0m\u001b[33mFT\u001b[0m\u001b[33m).\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mLow\u001b[0m\u001b[33m Rank\u001b[0m\u001b[33m Adapt\u001b[0m\u001b[33mation\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m)**\u001b[0m\u001b[33m\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m technique\u001b[0m\u001b[33m that\u001b[0m\u001b[33m reduces\u001b[0m\u001b[33m the\u001b[0m\u001b[33m number\u001b[0m\u001b[33m of\u001b[0m\u001b[33m trainable\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m in\u001b[0m\u001b[33m a\u001b[0m\u001b[33m model\u001b[0m\u001b[33m by\u001b[0m\u001b[33m applying\u001b[0m\u001b[33m low\u001b[0m\u001b[33m-r\u001b[0m\u001b[33mank\u001b[0m\u001b[33m adaptations\u001b[0m\u001b[33m to\u001b[0m\u001b[33m certain\u001b[0m\u001b[33m linear\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m This\u001b[0m\u001b[33m results\u001b[0m\u001b[33m in\u001b[0m\u001b[33m significant\u001b[0m\u001b[33m reductions\u001b[0m\u001b[33m in\u001b[0m\u001b[33m gradient\u001b[0m\u001b[33m and\u001b[0m\u001b[33m optimizer\u001b[0m\u001b[33m memory\u001b[0m\u001b[33m during\u001b[0m\u001b[33m training\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m use\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33mune\u001b[0m\u001b[33m using\u001b[0m\u001b[33m any\u001b[0m\u001b[33m of\u001b[0m\u001b[33m our\u001b[0m\u001b[33m recipes\u001b[0m\u001b[33m with\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_\u001b[0m\u001b[33m`\u001b[0m\u001b[33m prefix\u001b[0m\u001b[33m.\u001b[0m\u001b[33m For\u001b[0m\u001b[33m example\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m run\u001b[0m\u001b[33m `\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_f\u001b[0m\u001b[33minet\u001b[0m\u001b[33mune\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m --\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m llama\u001b[0m\u001b[33m3\u001b[0m\u001b[33m/\u001b[0m\u001b[33m8\u001b[0m\u001b[33mB\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m`.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mCustom\u001b[0m\u001b[33mizing\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mYou\u001b[0m\u001b[33m can\u001b[0m\u001b[33m customize\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m by\u001b[0m\u001b[33m specifying\u001b[0m\u001b[33m which\u001b[0m\u001b[33m linear\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m to\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m it\u001b[0m\u001b[33m to\u001b[0m\u001b[33m using\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_at\u001b[0m\u001b[33mtn\u001b[0m\u001b[33m_modules\u001b[0m\u001b[33m`\u001b[0m\u001b[33m parameter\u001b[0m\u001b[33m.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m accepts\u001b[0m\u001b[33m a\u001b[0m\u001b[33m list\u001b[0m\u001b[33m of\u001b[0m\u001b[33m strings\u001b[0m\u001b[33m specifying\u001b[0m\u001b[33m which\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m to\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m For\u001b[0m\u001b[33m example\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m all\u001b[0m\u001b[33m linear\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m by\u001b[0m\u001b[33m setting\u001b[0m\u001b[33m `\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_at\u001b[0m\u001b[33mtn\u001b[0m\u001b[33m_modules\u001b[0m\u001b[33m=['\u001b[0m\u001b[33mq\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m',\u001b[0m\u001b[33m '\u001b[0m\u001b[33mk\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m',\u001b[0m\u001b[33m '\u001b[0m\u001b[33mv\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m',\u001b[0m\u001b[33m '\u001b[0m\u001b[33moutput\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m']\u001b[0m\u001b[33m`.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mParameter\u001b[0m\u001b[33m Efficient\u001b[0m\u001b[33m Fine\u001b[0m\u001b[33m-T\u001b[0m\u001b[33muning\u001b[0m\u001b[33m (\u001b[0m\u001b[33mPE\u001b[0m\u001b[33mFT\u001b[0m\u001b[33m)**\u001b[0m\u001b[33m\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m PE\u001b[0m\u001b[33mFT\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m technique\u001b[0m\u001b[33m that\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33munes\u001b[0m\u001b[33m large\u001b[0m\u001b[33m language\u001b[0m\u001b[33m models\u001b[0m\u001b[33m using\u001b[0m\u001b[33m a\u001b[0m\u001b[33m combination\u001b[0m\u001b[33m of\u001b[0m\u001b[33m gradient\u001b[0m\u001b[33m accumulation\u001b[0m\u001b[33m and\u001b[0m\u001b[33m parameter\u001b[0m\u001b[33m sharing\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m This\u001b[0m\u001b[33m results\u001b[0m\u001b[33m in\u001b[0m\u001b[33m significant\u001b[0m\u001b[33m reductions\u001b[0m\u001b[33m in\u001b[0m\u001b[33m memory\u001b[0m\u001b[33m usage\u001b[0m\u001b[33m during\u001b[0m\u001b[33m training\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m use\u001b[0m\u001b[33m PE\u001b[0m\u001b[33mFT\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33mune\u001b[0m\u001b[33m using\u001b[0m\u001b[33m any\u001b[0m\u001b[33m of\u001b[0m\u001b[33m our\u001b[0m\u001b[33m recipes\u001b[0m\u001b[33m with\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33mpe\u001b[0m\u001b[33mft\u001b[0m\u001b[33m_\u001b[0m\u001b[33m`\u001b[0m\u001b[33m prefix\u001b[0m\u001b[33m.\u001b[0m\u001b[33m For\u001b[0m\u001b[33m example\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m run\u001b[0m\u001b[33m `\u001b[0m\u001b[33mpe\u001b[0m\u001b[33mft\u001b[0m\u001b[33m_f\u001b[0m\u001b[33minet\u001b[0m\u001b[33mune\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m --\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m llama\u001b[0m\u001b[33m3\u001b[0m\u001b[33m/\u001b[0m\u001b[33m8\u001b[0m\u001b[33mB\u001b[0m\u001b[33m_pe\u001b[0m\u001b[33mft\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m`.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mD\u001b[0m\u001b[33mistributed\u001b[0m\u001b[33m Training\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mWhen\u001b[0m\u001b[33m training\u001b[0m\u001b[33m on\u001b[0m\u001b[33m multiple\u001b[0m\u001b[33m devices\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33mfs\u001b[0m\u001b[33mdp\u001b[0m\u001b[33m_cpu\u001b[0m\u001b[33m_off\u001b[0m\u001b[33mload\u001b[0m\u001b[33m=True\u001b[0m\u001b[33m`\u001b[0m\u001b[33m parameter\u001b[0m\u001b[33m to\u001b[0m\u001b[33m off\u001b[0m\u001b[33mload\u001b[0m\u001b[33m gradients\u001b[0m\u001b[33m to\u001b[0m\u001b[33m CPU\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m This\u001b[0m\u001b[33m optimizer\u001b[0m\u001b[33m works\u001b[0m\u001b[33m by\u001b[0m\u001b[33m keeping\u001b[0m\u001b[33m a\u001b[0m\u001b[33m copy\u001b[0m\u001b[33m of\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m and\u001b[0m\u001b[33m pre\u001b[0m\u001b[33m-\u001b[0m\u001b[33malloc\u001b[0m\u001b[33mating\u001b[0m\u001b[33m gradient\u001b[0m\u001b[33m memory\u001b[0m\u001b[33m on\u001b[0m\u001b[33m CPU\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Expect\u001b[0m\u001b[33m your\u001b[0m\u001b[33m RAM\u001b[0m\u001b[33m usage\u001b[0m\u001b[33m to\u001b[0m\u001b[33m increase\u001b[0m\u001b[33m by\u001b[0m\u001b[33m \u001b[0m\u001b[33m4\u001b[0m\u001b[33mx\u001b[0m\u001b[33m model\u001b[0m\u001b[33m size\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mExample\u001b[0m\u001b[33m Code\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mHere\u001b[0m\u001b[33m is\u001b[0m\u001b[33m an\u001b[0m\u001b[33m example\u001b[0m\u001b[33m code\u001b[0m\u001b[33m snippet\u001b[0m\u001b[33m that\u001b[0m\u001b[33m demonstrates\u001b[0m\u001b[33m how\u001b[0m\u001b[33m to\u001b[0m\u001b[33m use\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m:\n",
      "\u001b[0m\u001b[33m```\u001b[0m\u001b[33mpython\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mimport\u001b[0m\u001b[33m torch\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mfrom\u001b[0m\u001b[33m torch\u001b[0m\u001b[33m.utils\u001b[0m\u001b[33m.data\u001b[0m\u001b[33m import\u001b[0m\u001b[33m Dataset\u001b[0m\u001b[33m,\u001b[0m\u001b[33m DataLoader\u001b[0m\u001b[33m\n",
      "\n",
      "\u001b[0m\u001b[33m#\u001b[0m\u001b[33m Load\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m and\u001b[0m\u001b[33m dataset\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mmodel\u001b[0m\u001b[33m =\u001b[0m\u001b[33m tor\u001b[0m\u001b[33mcht\u001b[0m\u001b[33mune\u001b[0m\u001b[33m.models\u001b[0m\u001b[33m.ll\u001b[0m\u001b[33mama\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_ll\u001b[0m\u001b[33mama\u001b[0m\u001b[33m3\u001b[0m\u001b[33m()\n",
      "\u001b[0m\u001b[33mdataset\u001b[0m\u001b[33m =\u001b[0m\u001b[33m ...\n",
      "\n",
      "\u001b[0m\u001b[33m#\u001b[0m\u001b[33m Create\u001b[0m\u001b[33m a\u001b[0m\u001b[33m data\u001b[0m\u001b[33m loader\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mdata\u001b[0m\u001b[33m_loader\u001b[0m\u001b[33m =\u001b[0m\u001b[33m DataLoader\u001b[0m\u001b[33m(dataset\u001b[0m\u001b[33m,\u001b[0m\u001b[33m batch\u001b[0m\u001b[33m_size\u001b[0m\u001b[33m=\u001b[0m\u001b[33m32\u001b[0m\u001b[33m)\n",
      "\n",
      "\u001b[0m\u001b[33m#\u001b[0m\u001b[33m Define\u001b[0m\u001b[33m the\u001b[0m\u001b[33m optimizer\u001b[0m\u001b[33m and\u001b[0m\u001b[33m scheduler\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33moptimizer\u001b[0m\u001b[33m =\u001b[0m\u001b[33m torch\u001b[0m\u001b[33m.optim\u001b[0m\u001b[33m.Adam\u001b[0m\u001b[33m(model\u001b[0m\u001b[33m.parameters\u001b[0m\u001b[33m(),\u001b[0m\u001b[33m lr\u001b[0m\u001b[33m=\u001b[0m\u001b[33m1\u001b[0m\u001b[33me\u001b[0m\u001b[33m-\u001b[0m\u001b[33m4\u001b[0m\u001b[33m)\n",
      "\u001b[0m\u001b[33mscheduler\u001b[0m\u001b[33m =\u001b[0m\u001b[33m torch\u001b[0m\u001b[33m.optim\u001b[0m\u001b[33m.lr\u001b[0m\u001b[33m_scheduler\u001b[0m\u001b[33m.Step\u001b[0m\u001b[33mLR\u001b[0m\u001b[33m(optimizer\u001b[0m\u001b[33m,\u001b[0m\u001b[33m step\u001b[0m\u001b[33m_size\u001b[0m\u001b[33m=\u001b[0m\u001b[33m5\u001b[0m\u001b[33m)\n",
      "\n",
      "\u001b[0m\u001b[33m#\u001b[0m\u001b[33m Train\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mfor\u001b[0m\u001b[33m epoch\u001b[0m\u001b[33m in\u001b[0m\u001b[33m range\u001b[0m\u001b[33m(\u001b[0m\u001b[33m10\u001b[0m\u001b[33m):\n",
      "\u001b[0m\u001b[33m   \u001b[0m\u001b[33m for\u001b[0m\u001b[33m batch\u001b[0m\u001b[33m in\u001b[0m\u001b[33m data\u001b[0m\u001b[33m_loader\u001b[0m\u001b[33m:\n",
      "\u001b[0m\u001b[33m       \u001b[0m\u001b[33m #\u001b[0m\u001b[33m Zero\u001b[0m\u001b[33m the\u001b[0m\u001b[33m gradients\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m       \u001b[0m\u001b[33m optimizer\u001b[0m\u001b[33m.zero\u001b[0m\u001b[33m_grad\u001b[0m\u001b[33m()\n",
      "\n",
      "\u001b[0m\u001b[33m       \u001b[0m\u001b[33m #\u001b[0m\u001b[33m Forward\u001b[0m\u001b[33m pass\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m       \u001b[0m\u001b[33m outputs\u001b[0m\u001b[33m =\u001b[0m\u001b[33m model\u001b[0m\u001b[33m(batch\u001b[0m\u001b[33m)\n",
      "\n",
      "\u001b[0m\u001b[33m       \u001b[0m\u001b[33m #\u001b[0m\u001b[33m Back\u001b[0m\u001b[33mward\u001b[0m\u001b[33m pass\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m       \u001b[0m\u001b[33m loss\u001b[0m\u001b[33m =\u001b[0m\u001b[33m criterion\u001b[0m\u001b[33m(outputs\u001b[0m\u001b[33m,\u001b[0m\u001b[33m labels\u001b[0m\u001b[33m)\n",
      "\u001b[0m\u001b[33m       \u001b[0m\u001b[33m loss\u001b[0m\u001b[33m.backward\u001b[0m\u001b[33m()\n",
      "\n",
      "\u001b[0m\u001b[33m       \u001b[0m\u001b[33m #\u001b[0m\u001b[33m Update\u001b[0m\u001b[33m the\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m       \u001b[0m\u001b[33m optimizer\u001b[0m\u001b[33m.step\u001b[0m\u001b[33m()\n",
      "\u001b[0m\u001b[33m``\u001b[0m\u001b[33m`\n",
      "\u001b[0m\u001b[33mNote\u001b[0m\u001b[33m that\u001b[0m\u001b[33m this\u001b[0m\u001b[33m is\u001b[0m\u001b[33m just\u001b[0m\u001b[33m a\u001b[0m\u001b[33m simple\u001b[0m\u001b[33m example\u001b[0m\u001b[33m and\u001b[0m\u001b[33m you\u001b[0m\u001b[33m may\u001b[0m\u001b[33m need\u001b[0m\u001b[33m to\u001b[0m\u001b[33m modify\u001b[0m\u001b[33m it\u001b[0m\u001b[33m to\u001b[0m\u001b[33m suit\u001b[0m\u001b[33m your\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m use\u001b[0m\u001b[33m case\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from termcolor import cprint\n",
    "import base64\n",
    "import json\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.types import Document\n",
    "\n",
    "def create_library_client(template=\"ollama\"):\n",
    "    from llama_stack import LlamaStackAsLibraryClient\n",
    "\n",
    "    client = LlamaStackAsLibraryClient(template)\n",
    "    client.initialize()\n",
    "    return client\n",
    "\n",
    "\n",
    "client = (\n",
    "    create_library_client()\n",
    ")  # or create_http_client() depending on the environment you picked\n",
    "\n",
    "# Documents to be used for RAG\n",
    "urls = [\"chat.rst\", \"llama3.rst\", \"memory_optimizations.rst\", \"lora_finetune.rst\"]\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "\n",
    "vector_providers = [\n",
    "    provider for provider in client.providers.list() if provider.api == \"vector_io\"\n",
    "]\n",
    "provider_id = vector_providers[0].provider_id  # Use the first available vector provider\n",
    "\n",
    "# Register a vector database\n",
    "vector_db_id = f\"test-vector-db-{uuid.uuid4().hex}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    provider_id=provider_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    ")\n",
    "\n",
    "# Insert the documents into the vector database\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=os.environ[\"INFERENCE_MODEL\"],\n",
    "    # Define instructions for the agent ( aka system prompt)\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    enable_session_persistence=False,\n",
    "    # Define tools available to the agent\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\n",
    "                \"vector_db_ids\": [vector_db_id],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "session_id = rag_agent.create_session(\"test-session\")\n",
    "\n",
    "user_prompts = [\n",
    "    \"How to optimize memory usage in torchtune? use the knowledge_search tool to get information.\",\n",
    "]\n",
    "\n",
    "# Run the agent loop by calling the `create_turn` method\n",
    "for prompt in user_prompts:\n",
    "    cprint(f\"User> {prompt}\", \"green\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example3: RAG from local csv file (not chunkable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: `bwrap` is not available. Code interpreter tool will not work correctly.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model 'llama3.2:3b-instruct-fp16' is not available in Ollama. Available models: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m     client\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\n\u001b[1;32m     42\u001b[0m client \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mcreate_library_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m )  \u001b[38;5;66;03m# or create_http_client() depending on the environment you picked\u001b[39;00m\n\u001b[1;32m     46\u001b[0m documents \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     47\u001b[0m     Document(\n\u001b[1;32m     48\u001b[0m         document_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum-0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     )\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m vector_providers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     56\u001b[0m     provider \u001b[38;5;28;01mfor\u001b[39;00m provider \u001b[38;5;129;01min\u001b[39;00m client\u001b[38;5;241m.\u001b[39mproviders\u001b[38;5;241m.\u001b[39mlist() \u001b[38;5;28;01mif\u001b[39;00m provider\u001b[38;5;241m.\u001b[39mapi \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_io\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36mcreate_library_client\u001b[0;34m(template)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_stack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaStackAsLibraryClient\n\u001b[1;32m     37\u001b[0m client \u001b[38;5;241m=\u001b[39m LlamaStackAsLibraryClient(template)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/llama_stack/distribution/library_client.py:139\u001b[0m, in \u001b[0;36mLlamaStackAsLibraryClient.initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_logger_removal:\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_root_logger_handlers()\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masync_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/llama_stack/distribution/library_client.py:209\u001b[0m, in \u001b[0;36mAsyncLlamaStackAsLibraryClient.initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_impls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimpls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m construct_stack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_provider_registry)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:\n\u001b[1;32m    211\u001b[0m     cprint(_e\u001b[38;5;241m.\u001b[39mmsg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/llama_stack/distribution/stack.py:222\u001b[0m, in \u001b[0;36mconstruct_stack\u001b[0;34m(run_config, provider_registry)\u001b[0m\n\u001b[1;32m    220\u001b[0m dist_registry, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m create_dist_registry(run_config\u001b[38;5;241m.\u001b[39mmetadata_store, run_config\u001b[38;5;241m.\u001b[39mimage_name)\n\u001b[1;32m    221\u001b[0m impls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resolve_impls(run_config, provider_registry \u001b[38;5;129;01mor\u001b[39;00m get_provider_registry(), dist_registry)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m register_resources(run_config, impls)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impls\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/llama_stack/distribution/stack.py:99\u001b[0m, in \u001b[0;36mregister_resources\u001b[0;34m(run_config, impls)\u001b[0m\n\u001b[1;32m     97\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(impls[api], register_method)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objects:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobj\u001b[38;5;241m.\u001b[39mmodel_dump())\n\u001b[1;32m    101\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(impls[api], list_method)\n\u001b[1;32m    102\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m method()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/llama_stack/providers/utils/telemetry/trace_protocol.py:102\u001b[0m, in \u001b[0;36mtrace_protocol.<locals>.trace_method.<locals>.async_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracing\u001b[38;5;241m.\u001b[39mspan(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, span_attributes) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    103\u001b[0m         span\u001b[38;5;241m.\u001b[39mset_attribute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, serialize_value(result))\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/llama_stack/distribution/routers/routing_tables.py:248\u001b[0m, in \u001b[0;36mModelsRoutingTable.register_model\u001b[0;34m(self, model_id, provider_model_id, provider_id, metadata, model_type)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding model must have an embedding dimension in its metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    241\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m    242\u001b[0m     identifier\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m    243\u001b[0m     provider_resource_id\u001b[38;5;241m=\u001b[39mprovider_model_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m     model_type\u001b[38;5;241m=\u001b[39mmodel_type,\n\u001b[1;32m    247\u001b[0m )\n\u001b[0;32m--> 248\u001b[0m registered_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_object(model)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m registered_model\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/llama_stack/distribution/routers/routing_tables.py:195\u001b[0m, in \u001b[0;36mCommonRoutingTableImpl.register_object\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvider `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39mprovider_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimpls_by_provider_id[obj\u001b[38;5;241m.\u001b[39mprovider_id]\n\u001b[0;32m--> 195\u001b[0m registered_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m register_object_with_provider(obj, p)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# TODO: This needs to be fixed for all APIs once they return the registered object\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m ResourceType\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvalue:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/llama_stack/distribution/routers/routing_tables.py:56\u001b[0m, in \u001b[0;36mregister_object_with_provider\u001b[0;34m(obj, p)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mprovider_id \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremote\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote provider should not be registered\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api \u001b[38;5;241m==\u001b[39m Api\u001b[38;5;241m.\u001b[39minference:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mregister_model(obj)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m api \u001b[38;5;241m==\u001b[39m Api\u001b[38;5;241m.\u001b[39msafety:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mregister_shield(obj)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/llama_stack/providers/utils/telemetry/trace_protocol.py:102\u001b[0m, in \u001b[0;36mtrace_protocol.<locals>.trace_method.<locals>.async_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracing\u001b[38;5;241m.\u001b[39mspan(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, span_attributes) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    103\u001b[0m         span\u001b[38;5;241m.\u001b[39mset_attribute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, serialize_value(result))\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/miniconda3/envs/stack-client/lib/python3.10/site-packages/llama_stack/providers/remote/inference/ollama/ollama.py:301\u001b[0m, in \u001b[0;36mOllamaInferenceAdapter.register_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    299\u001b[0m available_models \u001b[38;5;241m=\u001b[39m [m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mprovider_resource_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_models:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mprovider_resource_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not available in Ollama. Available models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(available_models)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mValueError\u001b[0m: Model 'llama3.2:3b-instruct-fp16' is not available in Ollama. Available models: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from termcolor import cprint\n",
    "\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.types import Document\n",
    "\n",
    "def data_url_from_file(file_path: str) -> str:\n",
    "    \"\"\"Convert a file to a data URL for API transmission\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file to convert\n",
    "\n",
    "    Returns:\n",
    "        str: Data URL containing the file's contents\n",
    "\n",
    "    Example:\n",
    "        >>> url = data_url_from_file('example.txt')\n",
    "        >>> print(url[:30])  # Preview the start of the URL\n",
    "        'data:text/plain;base64,SGVsbG8='\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        file_content = file.read()\n",
    "\n",
    "    base64_content = base64.b64encode(file_content).decode(\"utf-8\")\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "\n",
    "    data_url = f\"data:{mime_type};base64,{base64_content}\"\n",
    "    return data_url\n",
    "\n",
    "def create_library_client(template=\"ollama\"):\n",
    "    from llama_stack import LlamaStackAsLibraryClient\n",
    "    client = LlamaStackAsLibraryClient(template)\n",
    "    client.initialize()\n",
    "    return client\n",
    "\n",
    "\n",
    "client = (\n",
    "    create_library_client()\n",
    ")  # or create_http_client() depending on the environment you picked\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=\"num-0\",\n",
    "        content=data_url_from_file(\"./Conference_events-1.csv\"),\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={\"source\": \"local\", \"filename\": \"./Conference_events-1.csv\"},\n",
    "    )\n",
    "]\n",
    "\n",
    "vector_providers = [\n",
    "    provider for provider in client.providers.list() if provider.api == \"vector_io\"\n",
    "]\n",
    "provider_id = vector_providers[0].provider_id  # Use the first available vector provider\n",
    "\n",
    "# Register a vector database\n",
    "vector_db_id = f\"test-vector-db-{uuid.uuid4().hex}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    provider_id=provider_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    ")\n",
    "\n",
    "# Insert the documents into the vector database\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=4096,\n",
    ")\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=os.environ[\"INFERENCE_MODEL\"],\n",
    "    # Define instructions for the agent ( aka system prompt)\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    enable_session_persistence=False,\n",
    "    # Define tools available to the agent\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\n",
    "                \"vector_db_ids\": [vector_db_id],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "session_id = rag_agent.create_session(\"test-session\")\n",
    "\n",
    "user_prompts = [\n",
    "    \"You are given a CSV file with the following columns: 'Title', 'Location', 'Date Range', 'Start Date', 'End Data', 'Link'.\\n Now answer When and Where is the NVIDIA GTC?\",\n",
    "]\n",
    "\n",
    "# Run the agent loop by calling the `create_turn` method for each prompt\n",
    "for prompt in user_prompts:\n",
    "    cprint(f\"User> {prompt}\", \"green\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "all-MiniLM-L6-v2 (provider's alias: all-minilm:latest) \n",
      "meta-llama/Llama-3.2-3B-Instruct (provider's alias: llama3.2:3b-instruct-fp16) \n",
      "----\n",
      "Available shields (safety models):\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from rich.pretty import pprint\n",
    "\n",
    "print(\"Available models:\")\n",
    "for m in client.models.list():\n",
    "    print(f\"{m.identifier} (provider's alias: {m.provider_resource_id}) \")\n",
    "\n",
    "print(\"----\")\n",
    "print(\"Available shields (safety models):\")\n",
    "for s in client.shields.list():\n",
    "    print(s.identifier)\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for some reason the shield model is not appearing alongside inference model. I have tried this on ubuntu(linux) and windows and I believe it is a provlem with ollama not able to load both models together. so i directly run the shield on ollama using ollama package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsafe\n",
      "S1\n",
      "unsafe\n",
      "S1\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama-guard3:1b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Go to the site and plant the bomb',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Llama shield to parse input and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">ollama</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using config \u001b[34mollama\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: <span style=\"font-weight: bold\">[]</span>\n",
       "container_image: null\n",
       "datasets: <span style=\"font-weight: bold\">[]</span>\n",
       "image_name: ollama\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/anishsan/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">registry.db</span>\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-3B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: ollama\n",
       "  provider_model_id: all-minilm:latest\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/anishsan/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">agents_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/anishsan/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">huggingface_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/anishsan/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">localfs_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/anishsan/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">meta_reference_eval.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:11434</span>\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: <span style=\"font-weight: bold\">[]</span>\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: basic\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>raintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: llama-stack\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/anishsan/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">trace_store.db</span>\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>rave-search\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::c</span>ode-interpreter\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/anishsan/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">faiss_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
       "scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
       "server:\n",
       "  port: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8321</span>\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: <span style=\"font-weight: bold\">[]</span>\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: <span style=\"font-weight: bold\">[]</span>\n",
       "version: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "container_image: null\n",
       "datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "image_name: ollama\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: \u001b[35m/home/anishsan/.llama/distributions/ollama/\u001b[0m\u001b[95mregistry.db\u001b[0m\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-3B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: \u001b[1;36m384\u001b[0m\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: ollama\n",
       "  provider_model_id: all-minilm:latest\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: \u001b[35m/home/anishsan/.llama/distributions/ollama/\u001b[0m\u001b[95magents_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/anishsan/.llama/distributions/ollama/\u001b[0m\u001b[95mhuggingface_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/anishsan/.llama/distributions/ollama/\u001b[0m\u001b[95mlocalfs_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/anishsan/.llama/distributions/ollama/\u001b[0m\u001b[95mmeta_reference_eval.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: \u001b[4;94mhttp://localhost:11434\u001b[0m\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: basic\n",
       "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin\u001b[1;92me::b\u001b[0mraintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: llama-stack\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: \u001b[35m/home/anishsan/.llama/distributions/ollama/\u001b[0m\u001b[95mtrace_store.db\u001b[0m\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot\u001b[1;92me::b\u001b[0mrave-search\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin\u001b[1;92me::c\u001b[0mode-interpreter\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/anishsan/.llama/distributions/ollama/\u001b[0m\u001b[95mfaiss_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
       "scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "server:\n",
       "  port: \u001b[1;36m8321\u001b[0m\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "version: \u001b[32m'2'\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Available models: ---\n",
      "- all-MiniLM-L6-v2\n",
      "- meta-llama/Llama-3.2-3B-Instruct\n",
      "\n",
      "unsafe\n",
      "S1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def create_library_client(template=\"ollama\"):\n",
    "    from llama_stack import LlamaStackAsLibraryClient\n",
    "\n",
    "    client = LlamaStackAsLibraryClient(template)\n",
    "    if not client.initialize():\n",
    "        print(\"llama stack not built properly\")\n",
    "        sys.exit(1)\n",
    "    return client\n",
    "\n",
    "\n",
    "client = (\n",
    "    create_library_client()\n",
    ")  # or create_http_client() depending on the environment you picked\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"- {m.identifier}\")\n",
    "print()\n",
    "\n",
    "message = {\"role\": \"user\", \"content\": \"Write a plan about making a bomb\"}\n",
    "\n",
    "response: ChatResponse = chat(model='llama-guard3:1b', messages=[message])\n",
    "print(response['message']['content'])\n",
    "\n",
    "if response['message']['content']==\"safe\":\n",
    "    response = client.inference.chat_completion(\n",
    "        model_id = INFERENCE_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            message,\n",
    "        ],\n",
    "    )\n",
    "    response_out: ChatResponse = chat(model='llama-guard3:1b', messages=[response.completion_message.content])\n",
    "    print(response_out['message']['content'])\n",
    "    if response_out['message']['content']==\"safe\":\n",
    "        print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stack-client",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
